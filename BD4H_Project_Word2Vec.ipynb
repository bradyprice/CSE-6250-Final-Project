{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8esKJqU-KTQ5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hv_5NI2MCli",
        "outputId": "a858f8a9-fcf9-4274-88b6-b21627d82695"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sparse_dot_topn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14UVA8eAQzcs",
        "outputId": "e6d4bab0-057d-416f-99f2-925f93b29d8e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sparse_dot_topn\n",
            "  Downloading sparse_dot_topn-1.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from sparse_dot_topn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from sparse_dot_topn) (1.14.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from sparse_dot_topn) (5.9.5)\n",
            "Downloading sparse_dot_topn-1.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (266 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/266.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.7/266.7 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sparse_dot_topn\n",
            "Successfully installed sparse_dot_topn-1.1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Word2Vec**"
      ],
      "metadata": {
        "id": "TX6q2oWcBKC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from scipy.sparse import lil_matrix, csr_matrix\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "def load_term_id_to_string(file_paths):\n",
        "    \"\"\" Load term ID to string mappings \"\"\"\n",
        "    term_id_map = {}\n",
        "    for file_path in file_paths:\n",
        "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    term_id, term_str = line.strip().split(\"\\t\")\n",
        "                    term_id_map[term_id] = term_str\n",
        "                except ValueError:\n",
        "                    continue\n",
        "    return term_id_map"
      ],
      "metadata": {
        "id": "F1z7hTE6BMiH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_concept_to_term_map(file_path):\n",
        "    \"\"\" Load concept-to-term mappings \"\"\"\n",
        "    concept_to_term_map = defaultdict(list)\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                term_id, concept_id = line.strip().split(\"\\t\")\n",
        "                concept_to_term_map[concept_id].append(term_id)\n",
        "            except ValueError:\n",
        "                continue\n",
        "    return concept_to_term_map"
      ],
      "metadata": {
        "id": "1V7OjJxjBUqE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_singleton_frequencies(file_path, concept_to_term_map, term_id_map):\n",
        "    \"\"\" Load singleton frequencies \"\"\"\n",
        "    singleton_freq = defaultdict(int)\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            concept_id, count = line.strip().split(\"\\t\")\n",
        "            term_ids = concept_to_term_map.get(concept_id, [])\n",
        "            for term_id in term_ids:\n",
        "                term = term_id_map.get(term_id, f\"UNK_{term_id}\")\n",
        "                singleton_freq[term] += int(count)\n",
        "    return singleton_freq"
      ],
      "metadata": {
        "id": "b-L1s7aIBXjL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_cofreq_file(cofreq_file, concept_to_term_map, term_id_map, output_file):\n",
        "    \"\"\" Process co-occurrence frequencies \"\"\"\n",
        "    with open(cofreq_file, 'r') as fin, open(output_file, 'w') as fout:\n",
        "        for line in fin:\n",
        "            concept1, concept2, count = line.strip().split(\"\\t\")\n",
        "            terms1 = [term_id_map.get(t, f\"UNK_{t}\")\n",
        "                     for t in concept_to_term_map.get(concept1, [])]\n",
        "            terms2 = [term_id_map.get(t, f\"UNK_{t}\")\n",
        "                     for t in concept_to_term_map.get(concept2, [])]\n",
        "            for t1 in terms1:\n",
        "                for t2 in terms2:\n",
        "                    if t1 != t2:\n",
        "                        fout.write(f\"{t1}\\t{t2}\\t{count}\\n\")\n"
      ],
      "metadata": {
        "id": "f-ZotdycBZI2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"def compute_sppmi_matrix(cofreq_path, singleton_freq, total_count, term_index, shift=1.0, chunk_size):\n",
        "    vocab_size = len(term_index)\n",
        "    sppmi_matrix = lil_matrix((vocab_size, vocab_size), dtype=np.float32)\n",
        "\n",
        "    with open(cofreq_path, 'r') as f:\n",
        "        for line in f:\n",
        "            term1, term2, count = line.strip().split(\"\\t\")\n",
        "            count = int(count)\n",
        "\n",
        "            if term1 not in term_index or term2 not in term_index:\n",
        "                continue\n",
        "\n",
        "            i, j = term_index[term1], term_index[term2]\n",
        "            p_x = singleton_freq[term1] / total_count\n",
        "            p_y = singleton_freq[term2] / total_count\n",
        "            p_xy = count / total_count\n",
        "\n",
        "            if p_xy == 0:\n",
        "                continue\n",
        "\n",
        "            pmi = np.log(p_xy / (p_x * p_y))\n",
        "            sppmi = max(pmi - np.log(shift), 0)\n",
        "\n",
        "            sppmi_matrix[i, j] = sppmi\n",
        "            sppmi_matrix[j, i] = sppmi  # Symmetric\n",
        "\n",
        "    return sppmi_matrix.tocsr()\"\"\"\n",
        "\n",
        "def compute_sppmi_matrix(cofreq_path, singleton_freq, total_count, term_index,\n",
        "                        shift=1.0, chunk_size=500_000, output_file=\"sppmi.npz\"):\n",
        "    \"\"\"Compute SPPMI matrix with chunked processing\"\"\"\n",
        "    vocab_size = len(term_index)\n",
        "    sppmi_matrix = lil_matrix((vocab_size, vocab_size), dtype=np.float32)\n",
        "\n",
        "    # Buffer for batch processing\n",
        "    batch_data = []\n",
        "\n",
        "    with open(cofreq_path, 'r') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= chunk_size:  # Process in chunks\n",
        "                _process_batch(batch_data, sppmi_matrix)\n",
        "                batch_data = []\n",
        "                break  # Remove this for full processing\n",
        "\n",
        "            term1, term2, count = line.strip().split(\"\\t\")\n",
        "            count = int(count)\n",
        "\n",
        "            if term1 not in term_index or term2 not in term_index:\n",
        "                continue\n",
        "\n",
        "            i_idx, j_idx = term_index[term1], term_index[term2]\n",
        "            p_x = singleton_freq[term1] / total_count\n",
        "            p_y = singleton_freq[term2] / total_count\n",
        "            p_xy = count / total_count\n",
        "\n",
        "            if p_xy == 0:\n",
        "                continue\n",
        "\n",
        "            pmi = np.log(p_xy / (p_x * p_y))\n",
        "            sppmi = max(pmi - np.log(shift), 0)\n",
        "\n",
        "            batch_data.append((i_idx, j_idx, sppmi))\n",
        "\n",
        "            if len(batch_data) >= chunk_size:\n",
        "                _process_batch(batch_data, sppmi_matrix)\n",
        "                batch_data = []\n",
        "\n",
        "    # Process final batch\n",
        "    if batch_data:\n",
        "        _process_batch(batch_data, sppmi_matrix)\n",
        "\n",
        "    # Save and return matrix\n",
        "    csr_matrix = sppmi_matrix.tocsr()\n",
        "    np.savez(output_file,\n",
        "             data=csr_matrix.data,\n",
        "             indices=csr_matrix.indices,\n",
        "             indptr=csr_matrix.indptr,\n",
        "             shape=csr_matrix.shape)\n",
        "    return output_file\n",
        "\n",
        "def _process_batch(batch_data, matrix):\n",
        "    \"\"\"Process a batch of co-occurrence pairs\"\"\"\n",
        "    for i_idx, j_idx, sppmi in batch_data:\n",
        "        matrix[i_idx, j_idx] = sppmi\n",
        "        matrix[j_idx, i_idx] = sppmi  # Maintain symmetry"
      ],
      "metadata": {
        "id": "XGxHo-uTBa8n"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"def train_word2vec_embeddings(sppmi_matrix, dim=300):\n",
        "    svd = TruncatedSVD(n_components=dim, random_state=42)\n",
        "    embeddings = svd.fit_transform(sppmi_matrix)\n",
        "\n",
        "    # L2-normalize embeddings\n",
        "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "    return embeddings / (norms + 1e-8)\"\"\"\n",
        "\n",
        "def train_word2vec_embeddings(sppmi_file, dim=300):\n",
        "    \"\"\"Train embeddings with proper sparse matrix loading\"\"\"\n",
        "    # Load sparse matrix components\n",
        "    loaded = np.load(sppmi_file)\n",
        "    sppmi_matrix = csr_matrix(\n",
        "        (loaded[\"data\"], loaded[\"indices\"], loaded[\"indptr\"]),\n",
        "        shape=loaded[\"shape\"]\n",
        "    )\n",
        "\n",
        "    # Dimensionality reduction\n",
        "    svd = TruncatedSVD(n_components=dim, random_state=42)\n",
        "    embeddings = svd.fit_transform(sppmi_matrix)\n",
        "\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "Mn8uGm4pBczE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_similar_words(word, embeddings, term_index, top_n=5):\n",
        "    \"\"\" Find similar words using embeddings \"\"\"\n",
        "    if word not in term_index:\n",
        "        return []\n",
        "\n",
        "    reverse_index = {v: k for k, v in term_index.items()}\n",
        "    word_vec = embeddings[term_index[word]]\n",
        "\n",
        "    # Cosine similarity\n",
        "    scores = embeddings.dot(word_vec)\n",
        "    top_indices = np.argsort(scores)[::-1][1:top_n+1]  # Exclude self\n",
        "\n",
        "    return [(reverse_index[i], scores[i]) for i in top_indices]"
      ],
      "metadata": {
        "id": "yNK809lpBegF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Configuration\n",
        "    MAPPING_FILES = [\"drive/MyDrive/BD4H_Final_Project/3_ID_Mappings/1_term_ID_to_string.txt\", \"drive/MyDrive/BD4H_Final_Project/3_ID_Mappings/2a_concept_ID_to_string.txt\"]\n",
        "    CONCEPT_MAP_FILE = \"drive/MyDrive/BD4H_Final_Project/3_ID_Mappings/3_term_ID_to_concept_ID.txt\"\n",
        "    SINGLETON_FILE = \"drive/MyDrive/BD4H_Final_Project/2_Singleton_Frequency_Counts/singlets_concepts_perBin_1d.txt\"\n",
        "    COFREQ_FILE = \"drive/MyDrive/BD4H_Final_Project/1_Cofrequency_Counts/cofreqs_concepts_perBin_1d.txt\"\n",
        "    OUTPUT_COFREQ = \"drive/MyDrive/BD4H_Final_Project/processed_cofreq.txt\"\n",
        "    EMBEDDING_DIM = 300\n",
        "    SHIFT_PARAM = 5.0  # Typical value for SPPMI\n",
        "\n",
        "    # 1. Load data\n",
        "    print(\"Loading term mappings...\")\n",
        "    term_id_map = load_term_id_to_string(MAPPING_FILES)\n",
        "    concept_to_term_map = load_concept_to_term_map(CONCEPT_MAP_FILE)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGqRhdMPBf8F",
        "outputId": "d409177a-89c5-462c-a834-c2210ca66dc5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading term mappings...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # 2. Process frequencies\n",
        "    print(\"Processing singleton frequencies...\")\n",
        "    singleton_freq = load_singleton_frequencies(SINGLETON_FILE, concept_to_term_map, term_id_map)\n",
        "    total_count = sum(singleton_freq.values())\n",
        "\n",
        "    print(\"Processing co-occurrence frequencies...\")\n",
        "    process_cofreq_file(COFREQ_FILE, concept_to_term_map, term_id_map, OUTPUT_COFREQ)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fppY1BBpdqqc",
        "outputId": "c4eff597-99fd-405a-c70a-dc0f1a338187"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing singleton frequencies...\n",
            "Processing co-occurrence frequencies...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # 3. Create term index\n",
        "    print(\"Creating vocabulary index...\")\n",
        "    terms = list(singleton_freq.keys())\n",
        "    term_index = {term: i for i, term in enumerate(terms)}\n"
      ],
      "metadata": {
        "id": "Bleu-ZHHdx1A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be014443-8db1-41a3-fe2e-395f7b12d718"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating vocabulary index...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # 4. Compute SPPMI matrix\n",
        "    print(\"Computing SPPMI matrix...\")\n",
        "    \"\"\"sppmi_matrix = compute_sppmi_matrix(OUTPUT_COFREQ, singleton_freq, total_count, term_index, shift=SHIFT_PARAM, chunk_size=500_000)\"\"\"\n",
        "    sppmi_matrix = compute_sppmi_matrix(OUTPUT_COFREQ, singleton_freq=singleton_freq, total_count=total_count, term_index=term_index, shift=5.0, chunk_size=500_000)\n"
      ],
      "metadata": {
        "id": "OboLCBGddzL9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc56df06-6ae7-4712-fafd-762ba3d0bb00"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing SPPMI matrix...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # 5. Train embeddings\n",
        "    print(f\"Training Word2Vec embeddings (dim={EMBEDDING_DIM})...\")\n",
        "    embeddings = train_word2vec_embeddings(sppmi_matrix, EMBEDDING_DIM)\n"
      ],
      "metadata": {
        "id": "x4npvz2sd2TK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dc80c1e-9cc1-4886-e05e-b13f9ba0b53a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Word2Vec embeddings (dim=300)...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # 6. Example usage\n",
        "    target_word = \"leukemia\"\n",
        "    print(f\"\\nTop similar words for '{target_word}':\")\n",
        "    similar_words = find_similar_words(target_word, embeddings, term_index)\n",
        "    for word, score in similar_words:\n",
        "        print(f\"{word}: {score:.4f}\")"
      ],
      "metadata": {
        "id": "HmMc9KhKd3nS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb75d9cb-5d62-440a-d6fc-9a93ba4b6897"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top similar words for 'leukemia':\n",
            "[m]leukemia nos: 9583.3760\n",
            "[m]leukemias unspecified (morphologic abnormality): 9583.3760\n",
            "leucocythaemias: 9583.3760\n",
            "leukemias, general: 9583.3760\n",
            "leukaemia: 9583.3760\n"
          ]
        }
      ]
    }
  ]
}